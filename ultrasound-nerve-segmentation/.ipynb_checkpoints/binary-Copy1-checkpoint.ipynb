{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): CUDA is installed, but device gpu0 is not available  (error: Unable to get the number of gpus available: unknown error)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2016)\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, MaxoutDense\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, RemoteMonitor\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 128, 128\n",
    "nfolds = 10\n",
    "\n",
    "def get_im_cv2(path, img_rows, img_cols):\n",
    "    img = cv2.imread(path, 0)\n",
    "    resized = cv2.resize(img, (img_cols, img_rows), cv2.INTER_LINEAR)\n",
    "    return resized\n",
    "\n",
    "\n",
    "def load_train(img_rows, img_cols):\n",
    "    X_train = []\n",
    "    X_train_id = []\n",
    "    mask_train = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print('Read train images')\n",
    "    files = glob.glob(\"train/*[0-9].tif\")\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2(fl, img_rows, img_cols)\n",
    "        X_train.append(img)\n",
    "        X_train_id.append(flbase[:-4])\n",
    "        mask_path = \"train/\" + flbase[:-4] + \"_mask.tif\"\n",
    "        mask = get_im_cv2(mask_path, img_rows, img_cols)\n",
    "        mask_train.append(mask)\n",
    "\n",
    "    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return np.array(X_train), np.array(mask_train), np.array(X_train_id)\n",
    "\n",
    "\n",
    "def load_test(img_rows, img_cols):\n",
    "    print('Read test images')\n",
    "    files = glob.glob(\"test/*[0-9].tif\")\n",
    "    X_test = []\n",
    "    X_test_id = []\n",
    "    start_time = time.time()\n",
    "    for fl in files:\n",
    "        flbase = os.path.basename(fl)\n",
    "        img = get_im_cv2(fl, img_rows, img_cols)\n",
    "        X_test.append(img)\n",
    "        X_test_id.append(flbase[:-4])\n",
    "\n",
    "    print('Read test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n",
    "    return np.array(X_test), np.array(X_test_id)\n",
    "\n",
    "def read_and_normalize_train_data(img_rows, img_cols):\n",
    "    train_data, train_target, train_id = load_train(img_rows, img_cols)\n",
    "    # Convert to 0 or 1\n",
    "    train_target = np.sum(train_target, axis = (1, 2)) == 0\n",
    "    train_target = np_utils.to_categorical(train_target, 2)\n",
    "    train_target = train_target.astype('uint8')\n",
    "    train_data = train_data.astype('float32')\n",
    "    train_data /= 255\n",
    "    train_data = train_data.reshape(train_data.shape[0], 1, img_rows, img_cols)\n",
    "    print('Train shape:', train_data.shape)\n",
    "    print(train_data.shape[0], 'train samples')\n",
    "    return train_data, train_target, train_id\n",
    "\n",
    "\n",
    "def read_and_normalize_test_data(img_rows, img_cols):\n",
    "    test_data, test_id = load_test(img_rows, img_cols)\n",
    "    test_data = test_data.astype('float32')\n",
    "    test_data /= 255\n",
    "    test_data = test_data.reshape(test_data.shape[0], 1, img_rows, img_cols)\n",
    "    print('Test shape:', test_data.shape)\n",
    "    print(test_data.shape[0], 'test samples')\n",
    "    return test_data, test_id\n",
    "\n",
    "\n",
    "def create_model(img_rows, img_cols):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dropout(0.5, input_shape=(1, img_rows, img_cols)))\n",
    "    \n",
    "    model.add(Convolution2D(4, 7, 7, border_mode='same', W_regularizer=l2(0.00), subsample=(2, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Convolution2D(8, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(8, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(16, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Convolution2D(32, 3, 3, border_mode='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    #model.add(Convolution2D(128, 3, 3, border_mode='same', W_regularizer=l2(0.000)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(Convolution2D(128, 3, 3, border_mode='same', W_regularizer=l2(0.000)))\n",
    "    #model.add(Activation('relu'))\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    opt = SGD(0.05, 0.95, decay = 0.002, nesterov=True)\n",
    "    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def getPredScorePercent(train_target, predictions_valid):\n",
    "    perc = 0\n",
    "    for i in range(len(train_target)):\n",
    "        pred = predictions_valid[i][0] > 0.5\n",
    "        real = train_target[i][0] > 0.5\n",
    "        if real == pred:\n",
    "            perc += 1\n",
    "    perc /= len(train_target)\n",
    "    return perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read train images\n",
      "Read train data time: 47.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.5/site-packages/keras/utils/np_utils.py:16: VisibleDeprecationWarning: using a boolean instead of an integer will result in an error in the future\n",
      "  Y[i, y[i]] = 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (5635, 1, 128, 128)\n",
      "5635 train samples\n",
      "Read test images\n",
      "Read test data time: 34.47 seconds\n",
      "Test shape: (5508, 1, 128, 128)\n",
      "5508 test samples\n"
     ]
    }
   ],
   "source": [
    "train_data, train_target, train_id = read_and_normalize_train_data(img_rows, img_cols)\n",
    "test_data, test_id = read_and_normalize_test_data(img_rows, img_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train:  4436 4436\n",
      "Split valid:  1199 1199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /home/ubuntu/.theano/compiledir_Linux-4.4--generic-x86_64-with-debian-stretch-sid-x86_64-3.5.1-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6853 - acc: 0.5791 - val_loss: 0.6892 - val_acc: 0.5897\n",
      "Epoch 2/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6699 - acc: 0.5945 - val_loss: 0.6785 - val_acc: 0.5897\n",
      "Epoch 3/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6608 - acc: 0.6005 - val_loss: 0.6785 - val_acc: 0.5897\n",
      "Epoch 4/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6590 - acc: 0.6145 - val_loss: 0.6778 - val_acc: 0.5897\n",
      "Epoch 5/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6509 - acc: 0.6307 - val_loss: 0.6778 - val_acc: 0.5897\n",
      "Epoch 6/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6483 - acc: 0.6283 - val_loss: 0.6850 - val_acc: 0.5897\n",
      "Epoch 7/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6477 - acc: 0.6330 - val_loss: 0.6793 - val_acc: 0.5897\n",
      "Epoch 8/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6406 - acc: 0.6418 - val_loss: 0.6822 - val_acc: 0.5897\n",
      "Epoch 9/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6293 - acc: 0.6533 - val_loss: 0.7087 - val_acc: 0.5897\n",
      "Epoch 10/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6316 - acc: 0.6477 - val_loss: 0.6811 - val_acc: 0.5897\n",
      "Epoch 11/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6340 - acc: 0.6499 - val_loss: 0.6769 - val_acc: 0.5897\n",
      "Epoch 12/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6250 - acc: 0.6628 - val_loss: 0.6810 - val_acc: 0.5897\n",
      "Epoch 13/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6242 - acc: 0.6630 - val_loss: 0.6785 - val_acc: 0.5897\n",
      "Epoch 14/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6195 - acc: 0.6747 - val_loss: 0.6981 - val_acc: 0.5897\n",
      "Epoch 15/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6089 - acc: 0.6776 - val_loss: 0.7170 - val_acc: 0.5897\n",
      "Epoch 16/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6114 - acc: 0.6799 - val_loss: 0.7227 - val_acc: 0.5897\n",
      "Epoch 17/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6031 - acc: 0.6826 - val_loss: 0.7025 - val_acc: 0.5897\n",
      "Epoch 18/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6056 - acc: 0.6860 - val_loss: 0.7070 - val_acc: 0.5897\n",
      "Epoch 19/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.6015 - acc: 0.6891 - val_loss: 0.7131 - val_acc: 0.5897\n",
      "Epoch 20/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.5975 - acc: 0.6909 - val_loss: 0.7041 - val_acc: 0.5897\n",
      "Epoch 21/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.5980 - acc: 0.6864 - val_loss: 0.7133 - val_acc: 0.5897\n",
      "Epoch 22/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.5918 - acc: 0.7027 - val_loss: 0.6980 - val_acc: 0.5897\n",
      "Epoch 23/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.5817 - acc: 0.6991 - val_loss: 0.7471 - val_acc: 0.5897\n",
      "Epoch 24/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.5822 - acc: 0.7076 - val_loss: 0.6782 - val_acc: 0.5897\n",
      "Epoch 25/100\n",
      "4436/4436 [==============================] - 26s - loss: 0.5796 - acc: 0.7009 - val_loss: 0.7345 - val_acc: 0.5897\n",
      "Epoch 26/100\n",
      "  64/4436 [..............................] - ETA: 19s - loss: 0.6255 - acc: 0.6875"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "yfull_train = np.empty(train_data.shape[0])\n",
    "yfull_test = []\n",
    "patient = np.fromiter(map(lambda s: int(s.split('_')[0]), train_id), dtype=np.int)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    zoom_range=0.15,\n",
    ")\n",
    "\n",
    "model = create_model(img_rows, img_cols)\n",
    "\n",
    "for i in range(1, 5):\n",
    "    sel = (patient >= i * 10) & (patient < (i + 1) * 10)\n",
    "    X_train, X_valid = train_data[np.logical_not(sel)], train_data[sel]\n",
    "    Y_train, Y_valid = train_target[np.logical_not(sel)], train_target[sel] \n",
    "    print('Split train: ', len(X_train), len(Y_train))\n",
    "    print('Split valid: ', len(X_valid), len(Y_valid))\n",
    "    remote = RemoteMonitor(root='http://localhost:9000')\n",
    "    model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), nb_epoch=nb_epoch,\n",
    "                        verbose=1, validation_data=(X_valid, Y_valid),\n",
    "                        callbacks=[remote], samples_per_epoch=len(X_train))\n",
    "    predictions_valid = model.predict(X_valid, batch_size=batch_size, verbose=1)\n",
    "    score = log_loss(Y_valid, predictions_valid)\n",
    "    print('Score log_loss: ', score)\n",
    "    \n",
    "    yfull_train[sel] = predictions_valid\n",
    "    test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n",
    "    yfull_test.append(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_valid = get_validation_predictions(train_data, yfull_train)\n",
    "score = log_loss(train_target, predictions_valid)\n",
    "print(\"Log_loss train independent avg: \", score)\n",
    "print('Final log_loss: {}, rows: {} cols: {} nfolds: {} epoch: {}'.format(score, img_rows, img_cols, nfolds, nb_epoch))\n",
    "perc = getPredScorePercent(train_target, train_id, predictions_valid)\n",
    "print('Percent success: {}'.format(perc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
